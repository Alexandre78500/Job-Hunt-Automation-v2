# Job Hunter Automation - Plan de DÃ©veloppement

> Document de rÃ©fÃ©rence pour l'implÃ©mentation du projet par les agents IA.
> DerniÃ¨re mise Ã  jour : 3 fÃ©vrier 2026

---

## 1. Vue d'ensemble

### 1.1 Objectif
Automatiser la recherche d'emploi quotidienne en :
1. Scrapant les offres d'emploi depuis plusieurs sources
2. Comparant les offres au profil du candidat via un systÃ¨me de scoring hybride
3. Notifiant les offres pertinentes (â‰¥70% de match) sur Discord

### 1.2 Contexte d'exÃ©cution
- **Environnement** : VPS Linux avec OpenClaw (assistant IA open-source)
- **DÃ©clenchement** : Cron job quotidien Ã  9h00
- **IntÃ©gration** : API REST pour communication avec OpenClaw

### 1.3 Profil cible

| CritÃ¨re | Valeur |
|---------|--------|
| **Poste recherchÃ©** | Data Analyst / Consultant Data / Data Analyst Power BI |
| **Localisation** | Paris / Ãle-de-France |
| **Type de contrat** | CDI uniquement |
| **DisponibilitÃ©** | ImmÃ©diate |
| **Cible entreprises** | ESN / Conseil (diversitÃ© de missions) |

**Stack technique :**
- **Expert** : Power BI (DAX, Power Query), SQL avancÃ©
- **ConfirmÃ©** : Python, Excel
- **Connaissances** : Tableau, Snowflake, Azure, Talend

**Ne connaÃ®t PAS** : DBT, SAS, Dataiku, Google Analytics, Data Science/ML avancÃ©

---

## 2. Architecture technique

### 2.1 Stack technologique

| Composant | Technologie | Justification |
|-----------|-------------|---------------|
| Langage | Python 3.11+ | Excellent Ã©cosystÃ¨me scraping, facile Ã  maintenir |
| Base de donnÃ©es | SQLite | LÃ©ger, portable, suffisant pour le volume |
| API | FastAPI | Rapide, async, documentation auto (OpenAPI) |
| Scraping | requests + BeautifulSoup | LÃ©ger pour WTTJ |
| Email parsing | Google Gmail API | Fiable pour parser les alertes LinkedIn |
| Notifications | Discord Webhook | Simple, gratuit, intÃ©grÃ© Ã  l'Ã©cosystÃ¨me |

### 2.2 Diagramme de flux

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CRON (9h00 quotidien)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            main.py (Orchestrateur)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  WTTJ Scraper     â”‚           â”‚  LinkedIn Email   â”‚
        â”‚  (BeautifulSoup)  â”‚           â”‚  Parser (Gmail)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     DÃ©duplication (hash)      â”‚
                    â”‚   + Insertion SQLite (new)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PrÃ©-filtre mots-clÃ©s        â”‚
                    â”‚   (scoring algorithmique)     â”‚
                    â”‚   Ã‰limine les < 30%           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   API REST â† OpenClaw         â”‚
                    â”‚   (scoring IA des 30-100%)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Mise Ã  jour scores SQLite   â”‚
                    â”‚   Statut: scored              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Filtre final (â‰¥ 70%)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Discord Webhook             â”‚
                    â”‚   Statut: notified            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Structure du projet

```
Job-Hunter-Automation/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # Point d'entrÃ©e, orchestrateur
â”‚   â”‚
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_scraper.py        # Classe abstraite BaseScraper
â”‚   â”‚   â”œâ”€â”€ wttj_scraper.py        # Welcome to the Jungle
â”‚   â”‚   â””â”€â”€ linkedin_email.py      # Parser Gmail pour alertes LinkedIn
â”‚   â”‚
â”‚   â”œâ”€â”€ matcher/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ keyword_matcher.py     # Scoring algorithmique (prÃ©-filtre)
â”‚   â”‚   â””â”€â”€ ai_scorer.py           # Interface vers OpenClaw API
â”‚   â”‚
â”‚   â”œâ”€â”€ notifier/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ discord_notifier.py    # Webhook Discord
â”‚   â”‚
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py              # ModÃ¨les SQLAlchemy
â”‚   â”‚   â””â”€â”€ repository.py          # CRUD operations
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py              # FastAPI endpoints pour OpenClaw
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ deduplication.py       # GÃ©nÃ©ration hash, vÃ©rification doublons
â”‚       â”œâ”€â”€ config.py              # Chargement configuration
â”‚       â””â”€â”€ logger.py              # Configuration logging
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ profile.yaml               # Profil candidat (mots-clÃ©s, prÃ©fÃ©rences)
â”‚   â””â”€â”€ settings.yaml              # Configuration globale
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ jobs.db                    # Base SQLite (crÃ©Ã©e automatiquement)
â”‚
â”œâ”€â”€ credentials/
â”‚   â””â”€â”€ gmail_credentials.json     # OAuth Gmail (gitignore)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_scrapers.py
â”‚   â”œâ”€â”€ test_matcher.py
â”‚   â””â”€â”€ test_deduplication.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_gmail_oauth.py       # Script one-time pour auth Gmail
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ PLAN.md
â””â”€â”€ README.md
```

---

## 3. SchÃ©ma de la base de donnÃ©es

### 3.1 Table `jobs`

```sql
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    hash TEXT UNIQUE NOT NULL,              -- SHA256 pour dÃ©duplication
    
    -- Identifiants
    source TEXT NOT NULL,                   -- 'wttj', 'linkedin'
    external_id TEXT,                       -- ID sur le site source
    url TEXT NOT NULL,
    
    -- Informations de l'offre
    title TEXT NOT NULL,
    company TEXT NOT NULL,
    location TEXT,
    contract_type TEXT,                     -- 'CDI', 'CDD', 'Freelance', etc.
    salary_min INTEGER,
    salary_max INTEGER,
    description TEXT,                       -- Fiche de poste complÃ¨te
    
    -- Scoring
    keyword_score REAL,                     -- Score prÃ©-filtre (0-100)
    ai_score REAL,                          -- Score IA OpenClaw (0-100)
    final_score REAL,                       -- Score combinÃ© final
    ai_reasoning TEXT,                      -- Explication du score IA
    
    -- Statut et mÃ©tadonnÃ©es
    status TEXT DEFAULT 'new',              -- 'new', 'scored', 'notified', 'applied', 'ignored'
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    scored_at TIMESTAMP,
    notified_at TIMESTAMP,
    
    -- Index pour performance
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_source ON jobs(source);
CREATE INDEX idx_jobs_final_score ON jobs(final_score);
CREATE INDEX idx_jobs_created_at ON jobs(created_at);
```

### 3.2 Table `scrape_logs` (optionnel, pour monitoring)

```sql
CREATE TABLE scrape_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source TEXT NOT NULL,
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    jobs_found INTEGER DEFAULT 0,
    jobs_new INTEGER DEFAULT 0,
    jobs_duplicate INTEGER DEFAULT 0,
    error_message TEXT,
    status TEXT DEFAULT 'running'           -- 'running', 'success', 'failed'
);
```

---

## 4. Configuration

### 4.1 Fichier `config/profile.yaml`

```yaml
# Profil candidat pour le scoring

candidate:
  name: "Alexandre"
  title: "Data Analyst"
  experience_years: 2
  availability: "immediate"
  languages:
    - name: "FranÃ§ais"
      level: "native"
    - name: "Anglais"
      level: "professional"

# CritÃ¨res de recherche
search:
  job_titles:
    - "Data Analyst"
    - "Consultant Data"
    - "Data Analyst Power BI"
    - "Business Intelligence Analyst"
    - "BI Analyst"
    - "Analyste DonnÃ©es"
  
  locations:
    - "Paris"
    - "Ãle-de-France"
    - "92"  # Hauts-de-Seine
    - "75"  # Paris
    - "Remote"
    - "TÃ©lÃ©travail"
  
  contract_types:
    - "CDI"
  
  company_types_preferred:
    - "ESN"
    - "Conseil"
    - "Cabinet de conseil"

# CompÃ©tences pour le scoring par mots-clÃ©s
skills:
  # CompÃ©tences obligatoires (Ã©liminatoire si absent)
  required:
    - keyword: "Power BI"
      weight: 15
      aliases: ["PowerBI", "Power-BI"]
    - keyword: "SQL"
      weight: 12
      aliases: ["SQL Server", "PostgreSQL", "MySQL"]
  
  # CompÃ©tences importantes
  important:
    - keyword: "DAX"
      weight: 10
    - keyword: "Power Query"
      weight: 10
      aliases: ["M Language"]
    - keyword: "Python"
      weight: 8
    - keyword: "Excel"
      weight: 5
      aliases: ["VBA", "Tableaux croisÃ©s"]
    - keyword: "Tableau"
      weight: 6
      aliases: ["Tableau Software"]
    - keyword: "Snowflake"
      weight: 7
    - keyword: "Azure"
      weight: 6
      aliases: ["Azure Data Factory", "Azure Synapse"]
    - keyword: "Talend"
      weight: 5
  
  # CompÃ©tences bonus
  nice_to_have:
    - keyword: "Dashboard"
      weight: 4
      aliases: ["Dashboarding", "Tableaux de bord"]
    - keyword: "KPI"
      weight: 3
      aliases: ["Indicateurs"]
    - keyword: "Reporting"
      weight: 3
    - keyword: "Data Quality"
      weight: 4
      aliases: ["QualitÃ© des donnÃ©es"]
    - keyword: "ETL"
      weight: 4
    - keyword: "Data Warehouse"
      weight: 4
      aliases: ["DWH", "EntrepÃ´t de donnÃ©es"]
  
  # CompÃ©tences non maÃ®trisÃ©es (malus si requises)
  not_known:
    - keyword: "DBT"
      penalty: -10
    - keyword: "SAS"
      penalty: -8
    - keyword: "Dataiku"
      penalty: -8
    - keyword: "Google Analytics"
      penalty: -5
    - keyword: "Machine Learning"
      penalty: -5
      aliases: ["ML", "Deep Learning"]
    - keyword: "Data Science"
      penalty: -5

# Mots-clÃ©s Ã©liminatoires (offre ignorÃ©e si prÃ©sent)
exclusions:
  titles:
    - "Senior"      # Trop d'expÃ©rience requise
    - "Lead"
    - "Manager"
    - "Directeur"
    - "Head of"
  
  requirements:
    - "10 ans"
    - "10+ ans"
    - "8 ans"
    - "expÃ©rience significative"
  
  companies:
    []  # Ajouter ici les entreprises Ã  Ã©viter

# Mots-clÃ©s bonus
bonuses:
  - keyword: "Junior"
    bonus: 10
  - keyword: "ConfirmÃ©"
    bonus: 5
  - keyword: "2 ans"
    bonus: 5
  - keyword: "3 ans"
    bonus: 3
```

### 4.2 Fichier `config/settings.yaml`

```yaml
# Configuration globale de l'application

app:
  name: "Job Hunter Automation"
  version: "1.0.0"
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR

# Configuration base de donnÃ©es
database:
  path: "data/jobs.db"
  cleanup_days: 30  # Supprimer les offres > 30 jours

# Configuration scraping
scraping:
  # Welcome to the Jungle
  wttj:
    enabled: true
    base_url: "https://www.welcometothejungle.com"
    search_queries:
      - "Data Analyst"
      - "Consultant Data"
      - "Power BI"
    location: "Paris, France"
    contract_type: "CDI"
    max_pages: 5  # Pages max par requÃªte
    delay_between_requests: 2  # Secondes
  
  # LinkedIn (via alertes email)
  linkedin:
    enabled: true
    email_label: "LinkedIn Jobs"  # Label Gmail pour filtrer
    max_emails_per_run: 50

# Configuration scoring
scoring:
  # Seuils
  keyword_prefilter_threshold: 30   # Score min pour passer Ã  l'IA
  ai_scoring_threshold: 70          # Score min pour notification
  
  # PondÃ©ration score final
  weights:
    keyword_score: 0.3    # 30% score mots-clÃ©s
    ai_score: 0.7         # 70% score IA

# Configuration API (pour OpenClaw)
api:
  host: "0.0.0.0"
  port: 8000
  
  # Endpoints
  endpoints:
    get_pending_jobs: "/api/jobs/pending"      # Jobs Ã  scorer
    submit_scores: "/api/jobs/scores"          # Soumettre scores IA
    get_stats: "/api/stats"                    # Statistiques

# Configuration notifications
notifications:
  discord:
    enabled: true
    webhook_url: "${DISCORD_WEBHOOK_URL}"  # Via .env
    
    # Format du message
    embed_color: 0x00D166  # Vert
    include_fields:
      - score
      - company
      - location
      - contract
      - salary
      - ai_reasoning

# Configuration OpenClaw
openclaw:
  # Prompt systÃ¨me pour le scoring IA
  system_prompt: |
    Tu es un assistant spÃ©cialisÃ© dans l'Ã©valuation de la pertinence des offres d'emploi.
    Tu dois Ã©valuer si une offre correspond au profil d'un Data Analyst avec 2 ans d'expÃ©rience,
    expert en Power BI/DAX/SQL, basÃ© Ã  Paris, cherchant un CDI en ESN/Conseil.
    
    RÃ©ponds UNIQUEMENT en JSON avec ce format:
    {
      "score": <0-100>,
      "reasoning": "<explication courte en 1-2 phrases>"
    }
  
  # Timeout pour les appels
  timeout_seconds: 30
```

### 4.3 Fichier `.env.example`

```bash
# Discord
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/xxx/yyy

# Gmail OAuth (gÃ©nÃ©rÃ© par scripts/setup_gmail_oauth.py)
GMAIL_CREDENTIALS_PATH=credentials/gmail_credentials.json
GMAIL_TOKEN_PATH=credentials/gmail_token.json

# OpenClaw API (si nÃ©cessaire)
OPENCLAW_API_URL=http://localhost:3000

# Optionnel: Proxy pour scraping
# HTTP_PROXY=http://proxy:port
# HTTPS_PROXY=http://proxy:port
```

---

## 5. SpÃ©cifications des modules

### 5.1 Module `scrapers/base_scraper.py`

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional
from datetime import datetime

@dataclass
class JobOffer:
    """Structure standard pour une offre d'emploi."""
    source: str                      # 'wttj', 'linkedin'
    external_id: Optional[str]
    url: str
    title: str
    company: str
    location: Optional[str]
    contract_type: Optional[str]
    salary_min: Optional[int]
    salary_max: Optional[int]
    description: str
    scraped_at: datetime = None

class BaseScraper(ABC):
    """Classe abstraite pour tous les scrapers."""
    
    @property
    @abstractmethod
    def source_name(self) -> str:
        """Nom de la source (ex: 'wttj')."""
        pass
    
    @abstractmethod
    def scrape(self) -> List[JobOffer]:
        """RÃ©cupÃ¨re les offres d'emploi."""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """VÃ©rifie si la source est accessible."""
        pass
```

### 5.2 Module `scrapers/wttj_scraper.py`

**ResponsabilitÃ©s :**
- Construire les URLs de recherche avec les paramÃ¨tres (titre, localisation, contrat)
- Parser les pages de rÃ©sultats (liste des offres)
- Extraire les dÃ©tails de chaque offre (fiche complÃ¨te)
- GÃ©rer la pagination
- Respecter les dÃ©lais entre requÃªtes

**Points d'attention :**
- WTTJ a une API GraphQL non documentÃ©e qui peut Ãªtre utilisÃ©e
- Alternativement, parser le HTML avec BeautifulSoup
- GÃ©rer les erreurs 429 (rate limiting)

### 5.3 Module `scrapers/linkedin_email.py`

**ResponsabilitÃ©s :**
- Se connecter Ã  Gmail via OAuth 2.0
- RÃ©cupÃ©rer les emails avec le label "LinkedIn Jobs"
- Parser le contenu HTML des emails d'alerte LinkedIn
- Extraire : titre, entreprise, localisation, URL de l'offre
- Marquer les emails comme lus aprÃ¨s traitement

**Points d'attention :**
- NÃ©cessite configuration OAuth initiale (script one-time)
- Les emails LinkedIn ont un format HTML spÃ©cifique Ã  parser
- Stocker le token de refresh pour les exÃ©cutions futures

### 5.4 Module `matcher/keyword_matcher.py`

**Algorithme de scoring :**

```python
def calculate_keyword_score(job: JobOffer, profile: dict) -> float:
    """
    Calcule un score de 0 Ã  100 basÃ© sur les mots-clÃ©s.
    
    Algorithme:
    1. VÃ©rifier les exclusions (retourne 0 si match)
    2. Calculer les points positifs (skills required, important, nice_to_have)
    3. Appliquer les pÃ©nalitÃ©s (skills not_known)
    4. Appliquer les bonus (keywords bonus)
    5. Normaliser sur 100
    """
    score = 0
    max_possible_score = sum(skill['weight'] for category in ['required', 'important', 'nice_to_have'] 
                             for skill in profile['skills'].get(category, []))
    
    text_to_search = f"{job.title} {job.description}".lower()
    
    # 1. VÃ©rifier exclusions
    for exclusion in profile['exclusions']['titles']:
        if exclusion.lower() in job.title.lower():
            return 0
    
    # 2. Calculer points positifs
    for category in ['required', 'important', 'nice_to_have']:
        for skill in profile['skills'].get(category, []):
            keywords = [skill['keyword']] + skill.get('aliases', [])
            if any(kw.lower() in text_to_search for kw in keywords):
                score += skill['weight']
    
    # 3. Appliquer pÃ©nalitÃ©s
    for skill in profile['skills'].get('not_known', []):
        keywords = [skill['keyword']] + skill.get('aliases', [])
        if any(kw.lower() in text_to_search for kw in keywords):
            # PÃ©nalitÃ© seulement si c'est une exigence ("requis", "obligatoire")
            if is_required_in_context(text_to_search, keywords):
                score += skill['penalty']  # Valeur nÃ©gative
    
    # 4. Appliquer bonus
    for bonus in profile.get('bonuses', []):
        if bonus['keyword'].lower() in text_to_search:
            score += bonus['bonus']
    
    # 5. Normaliser
    normalized_score = max(0, min(100, (score / max_possible_score) * 100))
    return round(normalized_score, 1)
```

### 5.5 Module `matcher/ai_scorer.py`

**ResponsabilitÃ©s :**
- Exposer les offres prÃ©-filtrÃ©es via l'API REST
- Recevoir les scores d'OpenClaw
- Construire les prompts pour l'IA

**Format de prompt pour OpenClaw :**

```
Ã‰value cette offre d'emploi pour un Data Analyst Power BI (2 ans exp, Paris, CDI).

OFFRE:
- Titre: {title}
- Entreprise: {company}
- Localisation: {location}
- Contrat: {contract_type}
- Description: {description[:2000]}

PROFIL CANDIDAT:
- Expert: Power BI, DAX, Power Query, SQL
- ConnaÃ®t: Python, Tableau, Snowflake, Azure, Talend
- Ne connaÃ®t PAS: DBT, SAS, Dataiku, ML avancÃ©
- Cherche: ESN/Conseil, missions variÃ©es

RÃ©ponds en JSON: {"score": 0-100, "reasoning": "..."}
```

### 5.6 Module `api/routes.py`

**Endpoints FastAPI :**

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="Job Hunter API", version="1.0.0")

# --- ModÃ¨les Pydantic ---

class JobForScoring(BaseModel):
    id: int
    title: str
    company: str
    location: Optional[str]
    contract_type: Optional[str]
    description: str
    keyword_score: float

class ScoreSubmission(BaseModel):
    job_id: int
    ai_score: float
    reasoning: str

class BulkScoreSubmission(BaseModel):
    scores: List[ScoreSubmission]

# --- Endpoints ---

@app.get("/api/jobs/pending", response_model=List[JobForScoring])
async def get_pending_jobs():
    """
    Retourne les offres en attente de scoring IA.
    (status='new' et keyword_score >= seuil prÃ©-filtre)
    """
    pass

@app.post("/api/jobs/scores")
async def submit_scores(submission: BulkScoreSubmission):
    """
    ReÃ§oit les scores IA d'OpenClaw et met Ã  jour la DB.
    DÃ©clenche l'envoi Discord pour les scores >= 70%.
    """
    pass

@app.get("/api/stats")
async def get_stats():
    """
    Statistiques: offres aujourd'hui, scores moyens, etc.
    """
    pass

@app.post("/api/trigger-scrape")
async def trigger_scrape():
    """
    DÃ©clenche manuellement un cycle de scraping.
    """
    pass
```

### 5.7 Module `notifier/discord_notifier.py`

**Format du message Discord (embed) :**

```python
def build_discord_embed(job: Job) -> dict:
    """Construit l'embed Discord pour une offre."""
    return {
        "embeds": [{
            "title": f"ğŸ¯ {job.title}",
            "url": job.url,
            "color": 0x00D166,  # Vert
            "fields": [
                {"name": "ğŸ¢ Entreprise", "value": job.company, "inline": True},
                {"name": "ğŸ“ Localisation", "value": job.location or "Non prÃ©cisÃ©", "inline": True},
                {"name": "ğŸ“„ Contrat", "value": job.contract_type or "Non prÃ©cisÃ©", "inline": True},
                {"name": "ğŸ’° Salaire", "value": format_salary(job.salary_min, job.salary_max), "inline": True},
                {"name": "ğŸ“Š Score", "value": f"**{job.final_score}%**", "inline": True},
                {"name": "ğŸ¤– Analyse IA", "value": job.ai_reasoning[:200] if job.ai_reasoning else "N/A", "inline": False},
            ],
            "footer": {"text": f"Source: {job.source.upper()} | {job.scraped_at.strftime('%d/%m/%Y')}"},
        }]
    }
```

### 5.8 Module `utils/deduplication.py`

```python
import hashlib
from typing import Optional

def generate_job_hash(url: str, title: Optional[str] = None, company: Optional[str] = None) -> str:
    """
    GÃ©nÃ¨re un hash unique pour une offre.
    
    PrioritÃ©:
    1. URL (si disponible et valide)
    2. Combinaison titre + entreprise (fallback)
    """
    if url and url.startswith('http'):
        # Normaliser l'URL (supprimer paramÃ¨tres tracking)
        clean_url = normalize_url(url)
        content = clean_url
    elif title and company:
        content = f"{title.lower().strip()}|{company.lower().strip()}"
    else:
        raise ValueError("Impossible de gÃ©nÃ©rer un hash: URL ou titre+entreprise requis")
    
    return hashlib.sha256(content.encode()).hexdigest()

def normalize_url(url: str) -> str:
    """Supprime les paramÃ¨tres de tracking de l'URL."""
    from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
    
    parsed = urlparse(url)
    # Garder seulement les paramÃ¨tres essentiels
    params = parse_qs(parsed.query)
    essential_params = {k: v for k, v in params.items() 
                       if k not in ['utm_source', 'utm_medium', 'utm_campaign', 'ref', 'source']}
    
    clean = parsed._replace(query=urlencode(essential_params, doseq=True))
    return urlunparse(clean)
```

---

## 6. Ã‰tapes d'implÃ©mentation

### Phase 1 : Fondations (PrioritÃ© haute)

| # | TÃ¢che | Fichier(s) | Estimation |
|---|-------|------------|------------|
| 1.1 | CrÃ©er la structure du projet | Tous les dossiers et `__init__.py` | 15 min |
| 1.2 | Configurer `requirements.txt` | `requirements.txt` | 10 min |
| 1.3 | ImplÃ©menter le chargement de config | `src/utils/config.py` | 30 min |
| 1.4 | CrÃ©er les fichiers de configuration | `config/profile.yaml`, `config/settings.yaml` | 20 min |
| 1.5 | ImplÃ©menter les modÃ¨les SQLAlchemy | `src/database/models.py` | 30 min |
| 1.6 | ImplÃ©menter le repository (CRUD) | `src/database/repository.py` | 45 min |
| 1.7 | ImplÃ©menter la dÃ©duplication | `src/utils/deduplication.py` | 20 min |
| 1.8 | Configurer le logging | `src/utils/logger.py` | 15 min |

### Phase 2 : Scrapers (PrioritÃ© haute)

| # | TÃ¢che | Fichier(s) | Estimation |
|---|-------|------------|------------|
| 2.1 | ImplÃ©menter `BaseScraper` | `src/scrapers/base_scraper.py` | 20 min |
| 2.2 | ImplÃ©menter le scraper WTTJ | `src/scrapers/wttj_scraper.py` | 2h |
| 2.3 | Script setup OAuth Gmail | `scripts/setup_gmail_oauth.py` | 30 min |
| 2.4 | ImplÃ©menter le parser LinkedIn email | `src/scrapers/linkedin_email.py` | 1h30 |
| 2.5 | Tests unitaires scrapers | `tests/test_scrapers.py` | 1h |

### Phase 3 : SystÃ¨me de matching (PrioritÃ© haute)

| # | TÃ¢che | Fichier(s) | Estimation |
|---|-------|------------|------------|
| 3.1 | ImplÃ©menter le scoring par mots-clÃ©s | `src/matcher/keyword_matcher.py` | 1h |
| 3.2 | ImplÃ©menter l'interface IA | `src/matcher/ai_scorer.py` | 45 min |
| 3.3 | Tests unitaires matcher | `tests/test_matcher.py` | 45 min |

### Phase 4 : API et notifications (PrioritÃ© moyenne)

| # | TÃ¢che | Fichier(s) | Estimation |
|---|-------|------------|------------|
| 4.1 | ImplÃ©menter l'API FastAPI | `src/api/routes.py` | 1h |
| 4.2 | ImplÃ©menter le notifier Discord | `src/notifier/discord_notifier.py` | 30 min |
| 4.3 | Tests API | `tests/test_api.py` | 30 min |

### Phase 5 : Orchestration et finalisation (PrioritÃ© moyenne)

| # | TÃ¢che | Fichier(s) | Estimation |
|---|-------|------------|------------|
| 5.1 | ImplÃ©menter l'orchestrateur `main.py` | `src/main.py` | 1h |
| 5.2 | CrÃ©er le script de dÃ©marrage | `run.py` ou `Makefile` | 15 min |
| 5.3 | Documentation README | `README.md` | 30 min |
| 5.4 | Configuration cron pour VPS | Documentation | 15 min |

### Phase 6 : Tests et dÃ©ploiement (PrioritÃ© basse)

| # | TÃ¢che | Fichier(s) | Estimation |
|---|-------|------------|------------|
| 6.1 | Tests d'intÃ©gration complets | `tests/` | 1h |
| 6.2 | Test de bout en bout | Manuel | 30 min |
| 6.3 | DÃ©ploiement VPS | Scripts/Documentation | 1h |

---

## 7. DÃ©pendances (`requirements.txt`)

```
# Core
python-dotenv>=1.0.0
pyyaml>=6.0

# Database
sqlalchemy>=2.0.0

# Web scraping
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# Gmail API
google-auth>=2.22.0
google-auth-oauthlib>=1.0.0
google-api-python-client>=2.95.0

# API
fastapi>=0.103.0
uvicorn>=0.23.0
pydantic>=2.0.0

# HTTP client async (pour notifier)
httpx>=0.24.0

# Utilities
python-dateutil>=2.8.0

# Dev/Test
pytest>=7.4.0
pytest-asyncio>=0.21.0
```

---

## 8. Notes importantes pour l'implÃ©mentation

### 8.1 Gestion des erreurs

- Chaque scraper doit gÃ©rer ses propres erreurs sans faire crasher l'ensemble
- Logger toutes les erreurs avec contexte (source, URL, etc.)
- Retry automatique avec backoff exponentiel pour les erreurs rÃ©seau

### 8.2 Rate limiting

- WTTJ : 2 secondes minimum entre chaque requÃªte
- Gmail API : Respecter les quotas (250 requÃªtes/utilisateur/seconde)
- Discord : Max 30 requÃªtes/minute sur un webhook

### 8.3 SÃ©curitÃ©

- Ne jamais commiter les credentials (`credentials/` dans `.gitignore`)
- Variables sensibles uniquement via `.env`
- Valider toutes les entrÃ©es de l'API

### 8.4 Maintenance

- Nettoyer les offres > 30 jours automatiquement
- Rotation des logs (garder 7 jours)
- Monitoring des erreurs de scraping (les sites changent leur HTML)

---

## 9. Commandes utiles

```bash
# Installation
pip install -r requirements.txt

# Configurer Gmail OAuth (one-time)
python scripts/setup_gmail_oauth.py

# Lancer le scraping manuellement
python -m src.main --scrape-only

# Lancer l'API pour OpenClaw
python -m src.main --api-only

# Lancer le cycle complet
python -m src.main

# Tests
pytest tests/ -v

# Cron (sur VPS)
# Ajouter dans crontab -e :
0 9 * * * cd /path/to/job-hunter && /path/to/venv/bin/python -m src.main >> /var/log/job-hunter.log 2>&1
```

---

## 10. Ã‰volutions futures (hors scope v1)

- [ ] Auto-candidature (easy apply LinkedIn/WTTJ)
- [ ] Interface web pour visualiser les offres
- [ ] Scraper Indeed
- [ ] Scraper HelloWork
- [ ] Analyse des tendances (salaires, skills demandÃ©s)
- [ ] GÃ©nÃ©ration automatique de lettres de motivation
- [ ] IntÃ©gration calendrier pour les entretiens

---

*Ce document est la rÃ©fÃ©rence pour l'implÃ©mentation. Toute modification doit Ãªtre reflÃ©tÃ©e ici.*
